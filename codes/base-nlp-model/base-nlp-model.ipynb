{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure necessary resources are available\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.vocab: List[str] = []\n",
        "        self.word_to_idx: Dict[str, int] = {}\n",
        "        self.idx_to_word: Dict[int, str] = {}\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = text.lower().strip()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text\n",
        "\n",
        "    def lemmatize(self, text: str) -> List[str]:\n",
        "        doc = nlp(text)\n",
        "        return [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenizes and lemmatizes the cleaned text into words.\n",
        "        \"\"\"\n",
        "        text = self.clean_text(text)\n",
        "        return self.lemmatize(text)\n",
        "\n",
        "    def build_vocabulary(self, texts: List[str]):\n",
        "        \"\"\"\n",
        "        Builds vocabulary from a list of input texts using frequency-based ordering.\n",
        "        \"\"\"\n",
        "        word_freq = Counter()\n",
        "        for text in texts:\n",
        "            word_freq.update(self.tokenize(text))\n",
        "\n",
        "        self.vocab = [word for word, _ in word_freq.most_common()]\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}"
      ],
      "metadata": {
        "id": "EonQYZMdlCCS"
      },
      "id": "EonQYZMdlCCS",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter\n",
        "\n",
        "class TFIDFVectorizer:\n",
        "    def __init__(self, preprocessor: TextPreprocessor):\n",
        "        \"\"\"\n",
        "        Initializes the TFIDFVectorizer with an external TextPreprocessor instance.\n",
        "        \"\"\"\n",
        "        self.idf: Dict[str, float] = {}\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def fit(self, texts: List[str]):\n",
        "        \"\"\"\n",
        "        Computes the IDF values from a list of texts.\n",
        "        \"\"\"\n",
        "        self.preprocessor.build_vocabulary(texts)\n",
        "        doc_freq = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            doc_freq.update(set(self.preprocessor.tokenize(text)))\n",
        "\n",
        "        num_docs = len(texts)\n",
        "        self.idf = {word: math.log((num_docs + 1) / (freq + 1)) + 1\n",
        "                    for word, freq in doc_freq.items()}  # Smoothing applied\n",
        "\n",
        "    def transform(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Converts a single text document into a TF-IDF vector.\n",
        "        \"\"\"\n",
        "        tokens = self.preprocessor.tokenize(text)\n",
        "        word_counts = Counter(tokens)\n",
        "        vocab_size = len(self.preprocessor.vocab)\n",
        "\n",
        "        vector = np.zeros(vocab_size)\n",
        "\n",
        "        for word, count in word_counts.items():\n",
        "            idx = self.preprocessor.word_to_idx.get(word)\n",
        "            if idx is not None:\n",
        "                tf = count / len(tokens)\n",
        "                vector[idx] = tf * self.idf.get(word, 0)\n",
        "\n",
        "        return vector\n",
        "\n",
        "    def get_vocab_size(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the size of the vocabulary.\n",
        "        \"\"\"\n",
        "        return len(self.preprocessor.vocab)\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        \"\"\"\n",
        "        Initializes the neural network using Xavier initialization.\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Xavier/Glorot initialization\n",
        "        self.hidden_weights = np.random.randn(hidden_size, input_size) * np.sqrt(2 / input_size)\n",
        "        self.output_weights = np.random.randn(output_size, hidden_size) * np.sqrt(2 / hidden_size)\n",
        "        self.hidden_bias = np.zeros(hidden_size)\n",
        "        self.output_bias = np.zeros(output_size)\n",
        "\n",
        "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Prevent overflow\n",
        "\n",
        "    def sigmoid_derivative(self, x: np.ndarray) -> np.ndarray:\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, inputs: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        hidden_sum = np.dot(self.hidden_weights, inputs) + self.hidden_bias\n",
        "        hidden_outputs = self.sigmoid(hidden_sum)\n",
        "        output_sum = np.dot(self.output_weights, hidden_outputs) + self.output_bias\n",
        "        final_outputs = self.sigmoid(output_sum)\n",
        "        return hidden_outputs, final_outputs\n",
        "\n",
        "    def backward(self, inputs: np.ndarray, hidden_outputs: np.ndarray,\n",
        "                 final_outputs: np.ndarray, targets: np.ndarray, learning_rate: float = 0.1):\n",
        "        output_errors = targets - final_outputs\n",
        "        output_deltas = output_errors * self.sigmoid_derivative(final_outputs)\n",
        "        hidden_errors = np.dot(self.output_weights.T, output_deltas)\n",
        "        hidden_deltas = hidden_errors * self.sigmoid_derivative(hidden_outputs)\n",
        "\n",
        "        self.output_weights += learning_rate * np.outer(output_deltas, hidden_outputs)\n",
        "        self.output_bias += learning_rate * output_deltas\n",
        "        self.hidden_weights += learning_rate * np.outer(hidden_deltas, inputs)\n",
        "        self.hidden_bias += learning_rate * hidden_deltas\n",
        "\n",
        "\n",
        "class BugResolver:\n",
        "    def __init__(self, preprocessor: TextPreprocessor):\n",
        "        \"\"\"\n",
        "        Initializes the BugResolver with:\n",
        "        - A TF-IDF vectorizer for text processing.\n",
        "        - A neural network.\n",
        "        - A mapping between numeric solution labels and actual solutions.\n",
        "        \"\"\"\n",
        "        self.vectorizer = TFIDFVectorizer(preprocessor)\n",
        "        self.network = None  # Neural network will be initialized in prepare_data\n",
        "        self.solution_mapping: Dict[int, str] = {}  # Maps numeric indices to solution texts\n",
        "\n",
        "    def prepare_data(self, bug_reports: List[Dict[str, str]]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepares training data by converting bug descriptions into numerical vectors\n",
        "        and mapping solutions to one-hot encoded labels.\n",
        "        \"\"\"\n",
        "        descriptions = [report[\"description\"] for report in bug_reports]\n",
        "        solutions = [report[\"solution\"] for report in bug_reports]\n",
        "\n",
        "        self.vectorizer.fit(descriptions)\n",
        "\n",
        "        if self.network is None:\n",
        "            input_size = self.vectorizer.get_vocab_size()\n",
        "            hidden_size = min(150, input_size // 2)\n",
        "            num_solutions = len(set(solutions))\n",
        "            self.network = NeuralNetwork(input_size, hidden_size, num_solutions)\n",
        "\n",
        "        unique_solutions = list(set(solutions))\n",
        "        self.solution_mapping = {i: sol for i, sol in enumerate(unique_solutions)}\n",
        "\n",
        "        X = np.array([self.vectorizer.transform(desc) for desc in descriptions])\n",
        "        y = np.zeros((len(solutions), len(unique_solutions)))\n",
        "\n",
        "        for i, sol in enumerate(solutions):\n",
        "            y[i, unique_solutions.index(sol)] = 1.0\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def train(self, bug_reports: List[Dict[str, str]], epochs: int = 100):\n",
        "        \"\"\"\n",
        "        Trains the neural network using the provided bug reports.\n",
        "        \"\"\"\n",
        "        X, y = self.prepare_data(bug_reports)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_error = 0\n",
        "\n",
        "            for i in range(len(X)):\n",
        "                hidden_outputs, final_outputs = self.network.forward(X[i])\n",
        "                self.network.backward(X[i], hidden_outputs, final_outputs, y[i])\n",
        "                total_error += np.sum((y[i] - final_outputs) ** 2)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Error: {total_error/len(X):.4f}\")\n",
        "\n",
        "    def predict(self, bug_description: str) -> str:\n",
        "        \"\"\"\n",
        "        Predicts the solution for a given bug description.\n",
        "        \"\"\"\n",
        "        input_vector = self.vectorizer.transform(bug_description)\n",
        "        _, outputs = self.network.forward(input_vector)\n",
        "        predicted_idx = np.argmax(outputs)\n",
        "        return self.solution_mapping[predicted_idx]"
      ],
      "metadata": {
        "id": "PNzYxQvVmWR4"
      },
      "id": "PNzYxQvVmWR4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Do not change anything from here\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/My Drive/base-nlp-model-dataset.xlsx\"\n",
        "\n",
        "def load_data_from_excel():\n",
        "    \"\"\"Loads data from an Excel file stored in Google Drive.\"\"\"\n",
        "    df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
        "\n",
        "    # Check column names\n",
        "    expected_columns = {\"description\", \"input\"}\n",
        "    if not expected_columns.issubset(df.columns):\n",
        "        raise ValueError(f\"Expected columns {expected_columns}, but got {df.columns}\")\n",
        "\n",
        "    # Convert DataFrame to a list of dictionaries\n",
        "    bug_reports = df[[\"description\", \"input\"]].dropna().to_dict(orient=\"records\")\n",
        "\n",
        "    # Rename keys for consistency\n",
        "    for report in bug_reports:\n",
        "        report[\"description\"] = report.pop(\"description\")\n",
        "        report[\"solution\"] = report.pop(\"input\")\n",
        "\n",
        "    print(f\"Total bug reports loaded: {len(bug_reports)}\")\n",
        "    return bug_reports"
      ],
      "metadata": {
        "id": "quON4vL8uG7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b036860a-e58b-4cbc-8e97-30b04f6dccd3"
      },
      "id": "quON4vL8uG7s",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ce716532",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce716532",
        "outputId": "d74fd523-ca03-4021-b607-95dff3f8c641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total bug reports loaded: 141\n",
            "Epoch 10/50, Error: 0.9964\n",
            "Epoch 20/50, Error: 0.9935\n",
            "Epoch 30/50, Error: 0.9930\n",
            "Epoch 40/50, Error: 0.9928\n",
            "Epoch 50/50, Error: 0.9927\n",
            "$ uname -i\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    # Generate an enhanced dataset\n",
        "    full_dataset = load_data_from_excel()\n",
        "\n",
        "    # Initialize the BugResolver (TF-IDF + Neural Network)\n",
        "    resolver = BugResolver(TextPreprocessor())\n",
        "\n",
        "    # Train the resolver using the generated dataset\n",
        "    resolver.train(full_dataset, epochs=50)\n",
        "\n",
        "    # Sample bug description to test prediction\n",
        "    bug = \"Description of problem: When I am changing the special permissions (suid, sgid and sticky) in Fedora with the numeric method (chmod 2755 for example), is possible add the special permissions with numbers, but isn't possible to clear permissions. for example: is possible do chmod 4755 to add suid but if I use chmod 0755 the permission suid isn't remove. In other form, with chmod u-s the permission clear right. The problem is that using the numeric method only is possible add permission specials\"\n",
        "\n",
        "    # Predict the most relevant solution using the trained model\n",
        "    solution = resolver.predict(bug)\n",
        "\n",
        "    # Print the predicted solution\n",
        "    print(solution)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}